{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinbrick007/YouTube-Summarizer/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging face Model training and summarization"
      ],
      "metadata": {
        "id": "RWWuC0ufXbfD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoqbvoE7QW0f",
        "outputId": "b03ec0ae-06b9-414c-d2bb-d9104bd7285d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-1.0.3-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2025.4.26)\n",
            "Downloading youtube_transcript_api-1.0.3-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-1.0.3\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "!pip install youtube-transcript-api\n",
        "#!pip install groq\n",
        "!pip install openai\n",
        "\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "#from openai import OpenAI\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, pipeline\n",
        "import os\n",
        "\n",
        "import csv\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from google.auth import default\n",
        "import pandas as pd\n",
        "\n",
        "creds, _ = default()\n",
        "print(creds.get_cred_info)\n",
        "#gc = gspread.authorize(creds)\n",
        "\n",
        "# Load BART model and tokenizer once\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "count = 0\n",
        "\n",
        "def convert_dict_to_document(data_dict, filename=\"output_document.txt\"):\n",
        "  \"\"\"\n",
        "  Converts a Python dictionary into a simple text document.\n",
        "\n",
        "  Args:\n",
        "    data_dict: The dictionary to convert.\n",
        "    filename: The name of the output file.\n",
        "  \"\"\"\n",
        "  with open(filename, 'w', encoding='utf-8') as f:\n",
        "    for key, value in data_dict.items():\n",
        "      f.write(f\"--- {key} ---\\n\")\n",
        "      f.write(f\"{value}\\n\\n\")\n",
        "  print(f\"Document created: {filename}\")\n",
        "\n",
        "\n",
        "\n",
        "def extract_video_id(youtube_url):\n",
        "    if \"youtu.be\" in youtube_url:\n",
        "        video_id = youtube_url.split(\"/\")[-1].split(\"?\")[0]\n",
        "    elif \"youtube.com\" in youtube_url:\n",
        "        video_id = youtube_url.split(\"v=\")[-1].split(\"&\")[0]\n",
        "    else:\n",
        "        raise ValueError(\"Invalid YouTube URL format\")\n",
        "    return video_id\n",
        "\n",
        "\n",
        "def get_youtube_transcript(video_id):\n",
        "    try:\n",
        "        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "        transcript = ' '.join([entry['text'] for entry in transcript_list])\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        count+=1\n",
        "        raise Exception(f\"Error fetching transcript: {e}\")\n",
        "\n",
        "\n",
        "def get_summary(transcript):\n",
        "    inputs = tokenizer.encode(\"Summarize the following real estate transcript for potential buyers. \"\n",
        "    \"Focus on key highlights, location advantages, and investment value:\\n\\n\" + transcript,\n",
        "                              return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(\n",
        "        inputs, max_length=300, min_length=100, length_penalty=1.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "\n",
        "SHEET_NAME = 'youtube_links.xlsx'\n",
        "LINK_COLUMN = 'A'\n",
        "TITLE_COLUMN = 'A'\n",
        "\n",
        "myinsights = {} # Initialize myinsights here\n",
        "\n",
        "try:\n",
        "  df = pd.read_excel(SHEET_NAME, engine='openpyxl')\n",
        "  # View the column names (optional)\n",
        "  print(\"Available columns:\", df.columns)\n",
        "  links = df['Video URL'].dropna().tolist()\n",
        "\n",
        "  #sheet = gc.open(SHEET_NAME).get_worksheet(0)\n",
        "  # Get all values from the first row\n",
        "  #print(f\"\\nProcessing link: {sheet.row_values(2)}\")\n",
        "  # Get all values from the link column\n",
        "  #youtube_links = [sheet.col_values()]\n",
        "  youtube_links = links\n",
        "  #titles = [cell.value for cell in sheet.col_values(sheet.find(TITLE_COLUMN).col)]\n",
        "  #youtube_links = [\"https://www.youtube.com/watch?v=2KvL9lTcewA\"]\n",
        "  print(len(youtube_links))\n",
        "\n",
        "  for link in youtube_links:\n",
        "\n",
        "    if link: # Check if the cell is not empty\n",
        "      print(f\"\\nProcessing link: {link}\")\n",
        "      # Extract video ID from the link\n",
        "      try:\n",
        "          url_data = urlparse(link)\n",
        "          if url_data.hostname == 'youtu.be':\n",
        "              video_id = extract_video_id(link)\n",
        "          elif url_data.hostname in ('www.youtube.com', 'youtube.com'):\n",
        "              video_id = extract_video_id(link)\n",
        "          else:\n",
        "              video_id = None\n",
        "              print(\"Invalid YouTube link format.\")\n",
        "\n",
        "          if video_id:\n",
        "            transcript = get_youtube_transcript(video_id)\n",
        "            if transcript:\n",
        "              print(\"Transcript fetched successfully. Getting insights from Groq...\")\n",
        "              insights = get_summary(transcript)\n",
        "              #print(\"Insights :\",insights)\n",
        "              #print(insights)\n",
        "              myinsights[video_id] = insights\n",
        "              #convert_dict_to_document(myinsights, \"youtube_insights.txt\")\n",
        "            else:\n",
        "              print(\"Skipping Groq analysis due to missing transcript.\")\n",
        "          else:\n",
        "              print(\"Could not extract video ID from the link.\")\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Error processing link {link}: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"Error accessing Google Sheet: {e}\")\n",
        "\n",
        "print(myinsights)\n",
        "print(f\"\\Links unprocessed {count}\")\n",
        "#convert_dict_to_document(myinsights, \"youtube_insights.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Code-working**"
      ],
      "metadata": {
        "id": "WMjhH5DZXQDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: code to take youtube video links  from a csv file in runtime and download caption through API and pass it on to grok LLM through its API to get some insights out of it\n",
        "!pip install youtube-transcript-api\n",
        "!pip install groq\n",
        "!pip install openai\n",
        "\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "import csv\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def convert_dict_to_document(data_dict, filename=\"output_document.txt\"):\n",
        "  \"\"\"\n",
        "  Converts a Python dictionary into a simple text document.\n",
        "\n",
        "  Args:\n",
        "    data_dict: The dictionary to convert.\n",
        "    filename: The name of the output file.\n",
        "  \"\"\"\n",
        "  with open(filename, 'w', encoding='utf-8') as f:\n",
        "    for key, value in data_dict.items():\n",
        "      f.write(f\"--- {key} ---\\n\")\n",
        "      f.write(f\"{value}\\n\\n\")\n",
        "  print(f\"Document created: {filename}\")\n",
        "\n",
        "\n",
        "\n",
        "# Function to extract video ID from a YouTube link\n",
        "def extract_video_id(youtube_url):\n",
        "    \"\"\"Extracts the video ID from a YouTube URL.\"\"\"\n",
        "    try:\n",
        "        url_data = urlparse(youtube_url)\n",
        "        if url_data.hostname == 'youtu.be':\n",
        "            return url_data.path[1:]\n",
        "        elif url_data.hostname in ('www.youtube.com', 'youtube.com'):\n",
        "            query = parse_qs(url_data.query)\n",
        "            return query.get('v', [None])[0]\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting video ID from {youtube_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Replace 'your_youtube_links.csv' with the actual path to your CSV file\n",
        "csv_file_path = 'youtube.csv'\n",
        "myinsights = {}\n",
        "# Assuming the CSV has one column with YouTube links\n",
        "try:\n",
        "    with open(csv_file_path, mode='r', encoding='utf-8') as csvfile:\n",
        "        csv_reader = csv.reader(csvfile)\n",
        "        # Skip header row if present\n",
        "        # next(csv_reader)\n",
        "\n",
        "        for row in csv_reader:\n",
        "            if row: # Check if the row is not empty\n",
        "                youtube_link = row[0].strip() # Assuming the link is in the first column\n",
        "                if youtube_link:\n",
        "                    print(f\"\\nProcessing link: {youtube_link}\")\n",
        "                    video_id = extract_video_id(youtube_link)\n",
        "\n",
        "                    if video_id:\n",
        "                        transcript = get_youtube_transcript(video_id)\n",
        "                        if transcript:\n",
        "                            print(\"Transcript fetched successfully. Getting insights from Groq...\")\n",
        "                            insights = get_grok_insights(transcript)\n",
        "                            myinsights[video_id]=insights\n",
        "                            print(\"Insights from Groq:\")\n",
        "                            #print(insights)\n",
        "                        else:\n",
        "                            print(\"Skipping Groq analysis due to missing transcript.\")\n",
        "                    else:\n",
        "                        print(\"Could not extract video ID from the link.\")\n",
        "            else:\n",
        "                print(\"Skipping empty row.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{csv_file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error reading CSV file: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "# Replace with your Groq API key\n",
        "# You can get one from https://console.groq.com/keys\n",
        "API_KEY = 'xai-vikB5twp4LqLyqOoB25NzF8Qn8yWIZg5OlyO4xIgut7uK66cVZQuvacmSpUdSvS55lRvApmFPLVyv0VB'\n",
        "# Replace with the Groq model you want to use, e.g., \"llama3-8b-8192\" or \"llama3-70b-8192\"\n",
        "GROQ_MODEL = 'grok-3-mini-beta'\n",
        "\n",
        "client = OpenAI(api_key=API_KEY, base_url=\"https://api.x.ai/v1\")\n",
        "\n",
        "def get_youtube_transcript(video_id):\n",
        "  \"\"\"Fetches the transcript for a given YouTube video ID.\"\"\"\n",
        "  try:\n",
        "    transcript_list = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "    transcript = ' '.join([entry['text'] for entry in transcript_list])\n",
        "    return transcript\n",
        "  except Exception as e:\n",
        "    print(f\"Could not fetch transcript for video ID {video_id}: {e}\")\n",
        "    return None\n",
        "\n",
        "def get_grok_insights(transcript):\n",
        "  \"\"\"Sends the transcript to the Groq API for insights.\"\"\"\n",
        "  if transcript is None:\n",
        "    return \"No transcript available to analyze.\"\n",
        "\n",
        "  try:\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"\n",
        "                You are a smart YouTube transcript summarizer. Your job is to read the full transcript of a video and return:\n",
        "\n",
        "        1. A short summary of the overall video (5-6 sentences).\n",
        "        2. Bullet-pointed highlights of key takeaways, structured and concise.\n",
        "        3. (Optional) Timestamps if they are included in the transcript.\n",
        "\n",
        "        Guidelines:\n",
        "        - Focus on clarity and information value such as numbers and statistics mentioned.\n",
        "        - Ignore filler, repetition, or casual phrases.\n",
        "        - Highlight important ideas, tips, steps, or arguments.\n",
        "        - If the transcript is long, break it into parts and summarize each before merging.\n",
        "\n",
        "        If the transcript is very short, just extract the key ideas directly.\n",
        "\n",
        "        NEVER summarize the YouTube title or guess content — only work from the transcript provided.\n",
        "            \"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Give me the summary of this transcript given: \" + \" \"+transcript,\n",
        "            }\n",
        "        ],\n",
        "        model=GROQ_MODEL,\n",
        "        temperature=0.1 # Adjust max tokens as needed\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "  except Exception as e:\n",
        "    print(f\"Error calling Groq API: {e}\")\n",
        "    return \"Could not get insights from Groq.\"\n",
        "\n",
        "convert_dict_to_document(myinsights, \"youtube_insights.txt\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S_5eCr5rWfjd",
        "outputId": "471df9f6-c504-4e1c-cd12-2f400512f70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->youtube-transcript-api) (2025.4.26)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "\n",
            "Processing link: URL\n",
            "Could not extract video ID from the link.\n",
            "\n",
            "Processing link: https://youtu.be/Kd26vLt8M2E\n",
            "Could not fetch transcript for video ID Kd26vLt8M2E: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=Kd26vLt8M2E! This is most likely caused by:\n",
            "\n",
            "No transcripts were found for any of the requested language codes: ('en',)\n",
            "\n",
            "For this video (Kd26vLt8M2E) transcripts are available in the following languages:\n",
            "\n",
            "(MANUALLY CREATED)\n",
            "None\n",
            "\n",
            "(GENERATED)\n",
            " - hi (\"Hindi (auto-generated)\")[TRANSLATABLE]\n",
            "\n",
            "(TRANSLATION LANGUAGES)\n",
            " - ab (\"Abkhazian\")\n",
            " - aa (\"Afar\")\n",
            " - af (\"Afrikaans\")\n",
            " - ak (\"Akan\")\n",
            " - sq (\"Albanian\")\n",
            " - am (\"Amharic\")\n",
            " - ar (\"Arabic\")\n",
            " - hy (\"Armenian\")\n",
            " - as (\"Assamese\")\n",
            " - ay (\"Aymara\")\n",
            " - az (\"Azerbaijani\")\n",
            " - bn (\"Bangla\")\n",
            " - ba (\"Bashkir\")\n",
            " - eu (\"Basque\")\n",
            " - be (\"Belarusian\")\n",
            " - bho (\"Bhojpuri\")\n",
            " - bs (\"Bosnian\")\n",
            " - br (\"Breton\")\n",
            " - bg (\"Bulgarian\")\n",
            " - my (\"Burmese\")\n",
            " - ca (\"Catalan\")\n",
            " - ceb (\"Cebuano\")\n",
            " - zh-Hans (\"Chinese (Simplified)\")\n",
            " - zh-Hant (\"Chinese (Traditional)\")\n",
            " - co (\"Corsican\")\n",
            " - hr (\"Croatian\")\n",
            " - cs (\"Czech\")\n",
            " - da (\"Danish\")\n",
            " - dv (\"Divehi\")\n",
            " - nl (\"Dutch\")\n",
            " - dz (\"Dzongkha\")\n",
            " - en (\"English\")\n",
            " - eo (\"Esperanto\")\n",
            " - et (\"Estonian\")\n",
            " - ee (\"Ewe\")\n",
            " - fo (\"Faroese\")\n",
            " - fj (\"Fijian\")\n",
            " - fil (\"Filipino\")\n",
            " - fi (\"Finnish\")\n",
            " - fr (\"French\")\n",
            " - gaa (\"Ga\")\n",
            " - gl (\"Galician\")\n",
            " - lg (\"Ganda\")\n",
            " - ka (\"Georgian\")\n",
            " - de (\"German\")\n",
            " - el (\"Greek\")\n",
            " - gn (\"Guarani\")\n",
            " - gu (\"Gujarati\")\n",
            " - ht (\"Haitian Creole\")\n",
            " - ha (\"Hausa\")\n",
            " - haw (\"Hawaiian\")\n",
            " - iw (\"Hebrew\")\n",
            " - hi (\"Hindi\")\n",
            " - hmn (\"Hmong\")\n",
            " - hu (\"Hungarian\")\n",
            " - is (\"Icelandic\")\n",
            " - ig (\"Igbo\")\n",
            " - id (\"Indonesian\")\n",
            " - iu (\"Inuktitut\")\n",
            " - ga (\"Irish\")\n",
            " - it (\"Italian\")\n",
            " - ja (\"Japanese\")\n",
            " - jv (\"Javanese\")\n",
            " - kl (\"Kalaallisut\")\n",
            " - kn (\"Kannada\")\n",
            " - kk (\"Kazakh\")\n",
            " - kha (\"Khasi\")\n",
            " - km (\"Khmer\")\n",
            " - rw (\"Kinyarwanda\")\n",
            " - ko (\"Korean\")\n",
            " - kri (\"Krio\")\n",
            " - ku (\"Kurdish\")\n",
            " - ky (\"Kyrgyz\")\n",
            " - lo (\"Lao\")\n",
            " - la (\"Latin\")\n",
            " - lv (\"Latvian\")\n",
            " - ln (\"Lingala\")\n",
            " - lt (\"Lithuanian\")\n",
            " - lua (\"Luba-Lulua\")\n",
            " - luo (\"Luo\")\n",
            " - lb (\"Luxembourgish\")\n",
            " - mk (\"Macedonian\")\n",
            " - mg (\"Malagasy\")\n",
            " - ms (\"Malay\")\n",
            " - ml (\"Malayalam\")\n",
            " - mt (\"Maltese\")\n",
            " - gv (\"Manx\")\n",
            " - mi (\"Māori\")\n",
            " - mr (\"Marathi\")\n",
            " - mn (\"Mongolian\")\n",
            " - mfe (\"Morisyen\")\n",
            " - ne (\"Nepali\")\n",
            " - new (\"Newari\")\n",
            " - nso (\"Northern Sotho\")\n",
            " - no (\"Norwegian\")\n",
            " - ny (\"Nyanja\")\n",
            " - oc (\"Occitan\")\n",
            " - or (\"Odia\")\n",
            " - om (\"Oromo\")\n",
            " - os (\"Ossetic\")\n",
            " - pam (\"Pampanga\")\n",
            " - ps (\"Pashto\")\n",
            " - fa (\"Persian\")\n",
            " - pl (\"Polish\")\n",
            " - pt (\"Portuguese\")\n",
            " - pt-PT (\"Portuguese (Portugal)\")\n",
            " - pa (\"Punjabi\")\n",
            " - qu (\"Quechua\")\n",
            " - ro (\"Romanian\")\n",
            " - rn (\"Rundi\")\n",
            " - ru (\"Russian\")\n",
            " - sm (\"Samoan\")\n",
            " - sg (\"Sango\")\n",
            " - sa (\"Sanskrit\")\n",
            " - gd (\"Scottish Gaelic\")\n",
            " - sr (\"Serbian\")\n",
            " - crs (\"Seselwa Creole French\")\n",
            " - sn (\"Shona\")\n",
            " - sd (\"Sindhi\")\n",
            " - si (\"Sinhala\")\n",
            " - sk (\"Slovak\")\n",
            " - sl (\"Slovenian\")\n",
            " - so (\"Somali\")\n",
            " - st (\"Southern Sotho\")\n",
            " - es (\"Spanish\")\n",
            " - su (\"Sundanese\")\n",
            " - sw (\"Swahili\")\n",
            " - ss (\"Swati\")\n",
            " - sv (\"Swedish\")\n",
            " - tg (\"Tajik\")\n",
            " - ta (\"Tamil\")\n",
            " - tt (\"Tatar\")\n",
            " - te (\"Telugu\")\n",
            " - th (\"Thai\")\n",
            " - bo (\"Tibetan\")\n",
            " - ti (\"Tigrinya\")\n",
            " - to (\"Tongan\")\n",
            " - ts (\"Tsonga\")\n",
            " - tn (\"Tswana\")\n",
            " - tum (\"Tumbuka\")\n",
            " - tr (\"Turkish\")\n",
            " - tk (\"Turkmen\")\n",
            " - uk (\"Ukrainian\")\n",
            " - ur (\"Urdu\")\n",
            " - ug (\"Uyghur\")\n",
            " - uz (\"Uzbek\")\n",
            " - ve (\"Venda\")\n",
            " - vi (\"Vietnamese\")\n",
            " - war (\"Waray\")\n",
            " - cy (\"Welsh\")\n",
            " - fy (\"Western Frisian\")\n",
            " - wo (\"Wolof\")\n",
            " - xh (\"Xhosa\")\n",
            " - yi (\"Yiddish\")\n",
            " - yo (\"Yoruba\")\n",
            " - zu (\"Zulu\")\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "Skipping Groq analysis due to missing transcript.\n",
            "\n",
            "Processing link: https://youtu.be/bfzxj9BDokA\n",
            "Transcript fetched successfully. Getting insights from Groq...\n",
            "Insights from Groq:\n",
            "\n",
            "Processing link: https://youtu.be/Mi-r1uOXxXs\n",
            "Could not fetch transcript for video ID Mi-r1uOXxXs: \n",
            "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=Mi-r1uOXxXs! This is most likely caused by:\n",
            "\n",
            "No transcripts were found for any of the requested language codes: ('en',)\n",
            "\n",
            "For this video (Mi-r1uOXxXs) transcripts are available in the following languages:\n",
            "\n",
            "(MANUALLY CREATED)\n",
            "None\n",
            "\n",
            "(GENERATED)\n",
            " - hi (\"Hindi (auto-generated)\")[TRANSLATABLE]\n",
            "\n",
            "(TRANSLATION LANGUAGES)\n",
            " - ab (\"Abkhazian\")\n",
            " - aa (\"Afar\")\n",
            " - af (\"Afrikaans\")\n",
            " - ak (\"Akan\")\n",
            " - sq (\"Albanian\")\n",
            " - am (\"Amharic\")\n",
            " - ar (\"Arabic\")\n",
            " - hy (\"Armenian\")\n",
            " - as (\"Assamese\")\n",
            " - ay (\"Aymara\")\n",
            " - az (\"Azerbaijani\")\n",
            " - bn (\"Bangla\")\n",
            " - ba (\"Bashkir\")\n",
            " - eu (\"Basque\")\n",
            " - be (\"Belarusian\")\n",
            " - bho (\"Bhojpuri\")\n",
            " - bs (\"Bosnian\")\n",
            " - br (\"Breton\")\n",
            " - bg (\"Bulgarian\")\n",
            " - my (\"Burmese\")\n",
            " - ca (\"Catalan\")\n",
            " - ceb (\"Cebuano\")\n",
            " - zh-Hans (\"Chinese (Simplified)\")\n",
            " - zh-Hant (\"Chinese (Traditional)\")\n",
            " - co (\"Corsican\")\n",
            " - hr (\"Croatian\")\n",
            " - cs (\"Czech\")\n",
            " - da (\"Danish\")\n",
            " - dv (\"Divehi\")\n",
            " - nl (\"Dutch\")\n",
            " - dz (\"Dzongkha\")\n",
            " - en (\"English\")\n",
            " - eo (\"Esperanto\")\n",
            " - et (\"Estonian\")\n",
            " - ee (\"Ewe\")\n",
            " - fo (\"Faroese\")\n",
            " - fj (\"Fijian\")\n",
            " - fil (\"Filipino\")\n",
            " - fi (\"Finnish\")\n",
            " - fr (\"French\")\n",
            " - gaa (\"Ga\")\n",
            " - gl (\"Galician\")\n",
            " - lg (\"Ganda\")\n",
            " - ka (\"Georgian\")\n",
            " - de (\"German\")\n",
            " - el (\"Greek\")\n",
            " - gn (\"Guarani\")\n",
            " - gu (\"Gujarati\")\n",
            " - ht (\"Haitian Creole\")\n",
            " - ha (\"Hausa\")\n",
            " - haw (\"Hawaiian\")\n",
            " - iw (\"Hebrew\")\n",
            " - hi (\"Hindi\")\n",
            " - hmn (\"Hmong\")\n",
            " - hu (\"Hungarian\")\n",
            " - is (\"Icelandic\")\n",
            " - ig (\"Igbo\")\n",
            " - id (\"Indonesian\")\n",
            " - iu (\"Inuktitut\")\n",
            " - ga (\"Irish\")\n",
            " - it (\"Italian\")\n",
            " - ja (\"Japanese\")\n",
            " - jv (\"Javanese\")\n",
            " - kl (\"Kalaallisut\")\n",
            " - kn (\"Kannada\")\n",
            " - kk (\"Kazakh\")\n",
            " - kha (\"Khasi\")\n",
            " - km (\"Khmer\")\n",
            " - rw (\"Kinyarwanda\")\n",
            " - ko (\"Korean\")\n",
            " - kri (\"Krio\")\n",
            " - ku (\"Kurdish\")\n",
            " - ky (\"Kyrgyz\")\n",
            " - lo (\"Lao\")\n",
            " - la (\"Latin\")\n",
            " - lv (\"Latvian\")\n",
            " - ln (\"Lingala\")\n",
            " - lt (\"Lithuanian\")\n",
            " - lua (\"Luba-Lulua\")\n",
            " - luo (\"Luo\")\n",
            " - lb (\"Luxembourgish\")\n",
            " - mk (\"Macedonian\")\n",
            " - mg (\"Malagasy\")\n",
            " - ms (\"Malay\")\n",
            " - ml (\"Malayalam\")\n",
            " - mt (\"Maltese\")\n",
            " - gv (\"Manx\")\n",
            " - mi (\"Māori\")\n",
            " - mr (\"Marathi\")\n",
            " - mn (\"Mongolian\")\n",
            " - mfe (\"Morisyen\")\n",
            " - ne (\"Nepali\")\n",
            " - new (\"Newari\")\n",
            " - nso (\"Northern Sotho\")\n",
            " - no (\"Norwegian\")\n",
            " - ny (\"Nyanja\")\n",
            " - oc (\"Occitan\")\n",
            " - or (\"Odia\")\n",
            " - om (\"Oromo\")\n",
            " - os (\"Ossetic\")\n",
            " - pam (\"Pampanga\")\n",
            " - ps (\"Pashto\")\n",
            " - fa (\"Persian\")\n",
            " - pl (\"Polish\")\n",
            " - pt (\"Portuguese\")\n",
            " - pt-PT (\"Portuguese (Portugal)\")\n",
            " - pa (\"Punjabi\")\n",
            " - qu (\"Quechua\")\n",
            " - ro (\"Romanian\")\n",
            " - rn (\"Rundi\")\n",
            " - ru (\"Russian\")\n",
            " - sm (\"Samoan\")\n",
            " - sg (\"Sango\")\n",
            " - sa (\"Sanskrit\")\n",
            " - gd (\"Scottish Gaelic\")\n",
            " - sr (\"Serbian\")\n",
            " - crs (\"Seselwa Creole French\")\n",
            " - sn (\"Shona\")\n",
            " - sd (\"Sindhi\")\n",
            " - si (\"Sinhala\")\n",
            " - sk (\"Slovak\")\n",
            " - sl (\"Slovenian\")\n",
            " - so (\"Somali\")\n",
            " - st (\"Southern Sotho\")\n",
            " - es (\"Spanish\")\n",
            " - su (\"Sundanese\")\n",
            " - sw (\"Swahili\")\n",
            " - ss (\"Swati\")\n",
            " - sv (\"Swedish\")\n",
            " - tg (\"Tajik\")\n",
            " - ta (\"Tamil\")\n",
            " - tt (\"Tatar\")\n",
            " - te (\"Telugu\")\n",
            " - th (\"Thai\")\n",
            " - bo (\"Tibetan\")\n",
            " - ti (\"Tigrinya\")\n",
            " - to (\"Tongan\")\n",
            " - ts (\"Tsonga\")\n",
            " - tn (\"Tswana\")\n",
            " - tum (\"Tumbuka\")\n",
            " - tr (\"Turkish\")\n",
            " - tk (\"Turkmen\")\n",
            " - uk (\"Ukrainian\")\n",
            " - ur (\"Urdu\")\n",
            " - ug (\"Uyghur\")\n",
            " - uz (\"Uzbek\")\n",
            " - ve (\"Venda\")\n",
            " - vi (\"Vietnamese\")\n",
            " - war (\"Waray\")\n",
            " - cy (\"Welsh\")\n",
            " - fy (\"Western Frisian\")\n",
            " - wo (\"Wolof\")\n",
            " - xh (\"Xhosa\")\n",
            " - yi (\"Yiddish\")\n",
            " - yo (\"Yoruba\")\n",
            " - zu (\"Zulu\")\n",
            "\n",
            "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
            "Skipping Groq analysis due to missing transcript.\n",
            "\n",
            "Processing link: https://youtu.be/_B2QQ6qxoVE\n",
            "Transcript fetched successfully. Getting insights from Groq...\n",
            "Insights from Groq:\n",
            "\n",
            "Processing link: https://youtu.be/QFqLAwpEgHQ\n",
            "Transcript fetched successfully. Getting insights from Groq...\n",
            "Insights from Groq:\n",
            "\n",
            "Processing link: https://youtu.be/HU-NoMddjw4\n",
            "Transcript fetched successfully. Getting insights from Groq...\n",
            "Insights from Groq:\n",
            "\n",
            "Processing link: https://youtu.be/Wb_3lG8B_WE\n",
            "Could not fetch transcript for video ID Wb_3lG8B_WE: no element found: line 1, column 0\n",
            "Skipping Groq analysis due to missing transcript.\n",
            "\n",
            "Processing link: https://youtu.be/cZ0_bLcPnho\n",
            "Transcript fetched successfully. Getting insights from Groq...\n",
            "Insights from Groq:\n",
            "\n",
            "Processing link: https://youtu.be/LqtP6k3uTnk\n",
            "Transcript fetched successfully. Getting insights from Groq...\n",
            "Insights from Groq:\n",
            "\n",
            "Processing link: https://youtu.be/dDqDlghy5mQ\n",
            "Transcript fetched successfully. Getting insights from Groq...\n",
            "Insights from Groq:\n",
            "Document created: youtube_insights.txt\n"
          ]
        }
      ]
    }
  ]
}